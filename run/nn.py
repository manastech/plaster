"""
The "Archetype Classifier" is a classifier for fluorosequences using
K-Nearest-Neighbors with a Gaussian Mixture Model on top.

Terminology (applies to test_nn and test_nn)

    n_features:
        (n_cycles * n_channels)

    dye_count_space:
        The (n_cycles * n_channels) dimensional feature space of the data.

        Note that dye_count_space differs from "radiometry space" in that
        dye_count_space has unit brightness for each dye whereas radiometry_space
        has an independent brightness for each channel.

    dyemat:
        An instance of a matrix in dye_count_space

    dyerow:
        One row of a dyemat

    radiometry_space:
        The data space in which the data from the scope (or simulated) dwells.
        Like dye_count space this is an (n_cycles * n_channels) dimensional space,
        but wherein each channel has its own brightness and variance.

    radmat:
        An instance of a matrix in radiometry_space.
        Shape of n_rows * n_features

    radrow:
        One row of the radmat

    dye_gain:
        The brightness of each dye. Steps up-linearly with each additional dye-count.
        Note that this approximation may not hold for large numbers of dyes
        or different scopes/labels in that there is some evidence that single-molecule
        data should follow a log-normal distribution.

    variance_per_dye (VPD):
        The amount the variance goes up for each unit of dye. As noted above,
        there is some evidence that single molecule data should follow a log-normal
        distribution and therefore this model may be inadequate at higher dye counts.
        For now, this approximation seems sufficient.

    Gaussian Mixture Model (GMM):
        As the dye counts grows larger, the variance increases accordingly and thus
        individual fluoroseq classes tend to overlap at high dye_counts.
        The GMM is used to estimate the probability that a data-point came from
        a given fluoroseq.

    X:
        A matrix of n_rows by n_features holding sim or true signal data.

    uX:
        A matrix of X unit-normalized so that all channels are on a scale of 1 unit
        per dye.

    true_y:
    pred_y:
        A true or predicition vector of classification calls.


The NN classifier works by the following method:
    Calibration (off line):
        * Fits the ErrorModel (see error_model.py)

    Dye_tracks generated by monte-carlo simulations of peptides
    |
    |   Raw-data in radiometry-space from signal-processing or sim.
    |       |
    |       v
    +-> Find nearest-neighbor dye_track patterns within some neighbor-radius
            |
            v
        Weighted Gaussian Mixture Model to predict most likely dye_track in neighborhood.
            |
            v
        Maximum-Liklihood-Estimator to assign dye-track to peptide.


"""
from munch import Munch
import numpy as np
import pandas as pd
import pyflann
import random
from plaster.tools.utils import utils
from plaster.tools.schema import check
from plaster.tools.zap import zap
from plaster.tools.log.log import debug
from scipy.spatial import distance


def _create_flann(dt_mat):
    pyflann.set_distance_type("euclidean")
    flann = pyflann.FLANN()
    flann.build_index(utils.mat_flatter(dt_mat), algorithm="kdtree_simple")
    return flann


def _get_neighbor_iz(flann, radrow, n_neighbors, radius, default=0):
    """
    Return n_neighbors worth of neighbors using radius search.
    If unable to find n_neighbors worth, pad the return array with default
    so that it will always return np.array((n_neighbors,))
    """
    check.array_t(radrow, ndim=1)

    # I don't think there's any guarantee that if you ask
    # for only one neighbor that you get the closest one.
    # Therefore, best not to use this under that assumption.
    # assert n_neighbors > 1
    # Removing this assert for now because my test results
    # show that getting 1 neighbor gives me nearly identical
    # results so I'm considering switching to a nearest-neighbor
    # model alone.

    nn_iz, dists = flann.nn_radius(radrow.astype(float), radius, max_nn=n_neighbors)
    n_found = nn_iz.shape[0]
    neighbor_iz = np.full((n_neighbors,), default)
    neighbor_iz[0:n_found] = nn_iz[np.argsort(dists)]
    return neighbor_iz


def _do_nn_and_gmm(
    unit_radrow,
    dyerow,
    dt_mat,
    dt_inv_var_mat,
    dt_weights,
    flann,
    n_neighbors,
    dt_score_mode,
    dt_filter_threshold,
    dt_score_metric,
    dt_score_bias,
    penalty_coefs,
    rare_penalty,
    radius,
):
    """
    The workhorse of the Gaussian Mixture Model called from a parallel map below.

    Arguments:
        unit_radrow: One row of the unit_X matrix.
        dyerow: The true dyerow used for debugging
        dt_mat: The Dyetrack object used for neighbor lookup.
        dt_inv_var_mat: The inverse variance of each pattern (based on the VPD for each count)
        dt_weights: The weight of the pattern
        flann: The neighbor lookup
        The remainder are temporary parameters until I find the best solution

    Outline:
        unit_radmat_row is one row of uX. We ask the PYFLANN index to tell us the
        n nearest neighbors of all the dt_ann to this row and then for each
        of those neighbors we compute a variance-corrected distance.

    Notes:
        After some testing to compare to cdist "mahalanobis" I realized that actually
        the cdist function was not operating as expected. Specifically, it was taking the
        first element of the VI as THE ONLY covariance matrix, not one-per-dt! This means
        that all previous run results were wrong so I'm concerned how that was getting
        results that were as good as RF?
    """
    check.array_t(dt_mat, ndim=3)
    n_dts, n_channels, n_cycles = dt_mat.shape
    check.array_t(dt_inv_var_mat, shape=dt_mat.shape)
    check.array_t(unit_radrow, shape=(n_channels, n_cycles))
    check.array_t(dt_weights, shape=(n_dts,))

    n_cols = n_channels * n_cycles
    unit_radmat_row_flat = unit_radrow.reshape((n_cols,))

    # true_dt_i is found by use the "dyerow" which is really a cheat
    # since dyerow is oly something we know in simulated data.
    # There is no guarantee that the training set has every row
    # that is represented by the test set. Thus the "true_dt_i"
    # might not be found. We find it by using the _get_neighbor_iz
    # with a tiny radius because if the test dyetrack is found
    # among the targets it should be right on top of its target.
    true_dt_i = _get_neighbor_iz(
        flann,
        dyerow.reshape((n_cols,)),
        n_neighbors=n_neighbors,
        radius=0.01,
        default=0,
    )[0]

    nn_iz = _get_neighbor_iz(
        flann, unit_radmat_row_flat, n_neighbors=n_neighbors, radius=radius, default=0
    )
    nn_iz = nn_iz[nn_iz > 0]

    # FILTER any low-weight dyetracks
    sufficient_weight_mask = dt_weights[nn_iz] >= dt_filter_threshold
    nn_iz = nn_iz[sufficient_weight_mask]
    n_neigh_found = np.sum(nn_iz > 0)

    if n_neigh_found > 0:
        assert 1 <= n_neigh_found <= n_neighbors
        neigh_dt_mat = dt_mat[nn_iz].reshape((n_neigh_found, n_cols))
        neigh_dt_inv_var = dt_inv_var_mat[nn_iz].reshape((n_neigh_found, n_cols))
        neigh_weights = dt_weights[nn_iz]
        vdist = np.zeros_like(neigh_dt_inv_var)

        def cd():
            return distance.cdist(
                unit_radmat_row_flat.reshape((1, n_cols)),
                neigh_dt_mat,
                metric=dt_score_metric,
            )[0]

        def penalty():
            p = 1.0
            if rare_penalty is not None:
                p *= 1.0 - np.exp(-rare_penalty * neigh_weights)

            if penalty_coefs is not None:
                # Experimental: reduce score by total est. dye count
                total_brightness = unit_radrow.sum()
                # From fitting on RF I see that p_correct is correlated to
                # total brightness. A linear function m=0.054, b=0.216
                # So I'm going to try reducing score by this factor
                correction = max(
                    0.0,
                    min(1.0, (total_brightness * penalty_coefs[0] + penalty_coefs[1])),
                )
                assert 0 <= correction <= 1.0
                p *= correction

            return p

        if dt_score_mode == "gmm_normalized_wpdf":
            delta = unit_radmat_row_flat - neigh_dt_mat
            vdist = np.sum(delta * delta * neigh_dt_inv_var, axis=1)
            pdf = np.exp(-vdist)
            weighted_pdf = neigh_weights * pdf
            scores = utils.np_safe_divide(weighted_pdf, weighted_pdf.sum())

        elif dt_score_mode == "gmm_normalized_wpdf_dist_sigma":
            """
            https://en.wikipedia.org/wiki/Multivariate_normal_distribution
            Given:
                Sigma: covariance matrix
                mu: mean
                k: Dimensionality of the Gaussian
            The multivariate normal has the form:
                (
                    (2.0 * np.pi)**(-k / 2.0)
                ) * (
                    np.linalg.det(Sigma)**(-1.0 / 2.0)
                ) * np.exp(
                    -1.0 / 2.0 * ((x-mu).T @ np.linalg.inv(Sigma) @ (x-mu))
                )

            We can make some simplifications here:

            1.  We have n_rows (number of neighbors) and n_cols (number of feature dimensions)

            2.  We have pre-computed (x-mu) and call it "delta"
                This is a (n_rows, n_cols) matrix 

            3.  We don't actually have Sigma, rather we have 
                the "inverse variance" which we call: "neigh_dt_inv_var"
                and which we store in vector form!
                Therefore:
                   Sigma = 1.0 / neigh_dt_inv_var
                This is a (n_rows, n_cols) matrix

            4.  Our covariance matrix (Sigma) is diagonal and therefore
                its determinant is the product of the diagonal elements
                which, again, is stored as the rows.
                   np.linalg.det(Sigma) == np.prod(Sigma, axis=1)

            5.  Following from Sigma being diagonal, its inverse is:
                (1.0 / its elements). Therefore:
                   np.linalg.inv(Sigma) == 1.0 / Sigma

            6.  Furthermore, the term:
                   (x-mu).T @ np.linalg.inv(Sigma) @ (x-mu)
                is typically a full matrix expression.
                But Sigma is diagonal and stored as a vector, therefore:
                    delta.T @ np.sum((1.0 / Sigma) * delta, axis=1) ==
                    np.sum(delta * ((1.0 / Sigma) * delta), axis=1)

            7.  Because the whole equation converts to vector form
                all of the row operations on each neighbor can
                be done simultaneously.    

            8.  The  (2.0 * np.pi)**(-k / 2.0) is omitted as it will get 
                factored out when scores are computed.

            Therefore:
                n_rows = n_neigh_found = # Number of rows of neigh_dt_mat
                n_cols = # Number of columns of neigh_dt_mat
                delta = unit_radmat_row_flat - neigh_dt_mat  # np.array(size=(n_rows, n_cols,))
                Sigma = 1.0 / neigh_dt_inv_var  # np.array(size=(n_rows, n_cols,))

                pdf_at_x = (
                (2.0 * np.pi)**(-n_cols / 2.0)  # A constant (we can factor this out)
                ) * (
                np.prod(Sigma, axis=1)**(-1.0 / 2.0)  # np.array(size=(n_rows,))
                ) * np.exp(
                -1.0 / 2.0 * np.sum(delta * (neigh_dt_inv_var * delta), axis=1)
                )  # np.array(size=(n_rows,))
            """
            delta = unit_radmat_row_flat - neigh_dt_mat
            vdist = np.sum(delta * neigh_dt_inv_var * delta, axis=1)
            sigma = 1.0 / neigh_dt_inv_var
            determinant_of_sigma = np.prod(sigma, axis=1)
            pdf = determinant_of_sigma ** (-1 / 2) * np.exp(-vdist / 2)
            weighted_pdf = neigh_weights * pdf
            scores = utils.np_safe_divide(weighted_pdf, weighted_pdf.sum())

        elif dt_score_mode == "gmm_normalized_wpdf_no_inv_var":
            delta = unit_radmat_row_flat - neigh_dt_mat
            vdist = np.sum(delta * delta, axis=1)
            pdf = np.exp(-vdist)
            weighted_pdf = neigh_weights * pdf
            scores = utils.np_safe_divide(weighted_pdf, weighted_pdf.sum())

        elif dt_score_mode == "cdist_normalized":
            d = cd()
            scores = 1.0 / (dt_score_bias + d)
            scores = utils.np_safe_divide(scores, scores.sum())

        elif dt_score_mode == "cdist_weighted_sqrt":
            d = cd()
            scores = np.sqrt(neigh_weights) / (dt_score_bias + d)

        elif dt_score_mode == "cdist_weighted_log":
            d = cd()
            scores = np.log(neigh_weights) / (dt_score_bias + d)

        elif dt_score_mode == "cdist_weighted_normalized":
            d = cd()
            scores = neigh_weights / (dt_score_bias + d)
            scores = utils.np_safe_divide(scores, scores.sum())

        elif dt_score_mode == "cdist_weighted_normalized_sqrt":
            d = cd()
            scores = np.sqrt(neigh_weights) / (dt_score_bias + d)
            scores = utils.np_safe_divide(scores, scores.sum())

        elif dt_score_mode == "cdist_weighted_normalized_log":
            d = cd()
            scores = np.log(neigh_weights) / (dt_score_bias + d)
            scores = utils.np_safe_divide(scores, scores.sum())

        else:
            raise NotImplementedError()

        # PICK highest score
        scores *= penalty()
        arg_sort = np.argsort(scores)[::-1]
        best_arg = arg_sort[0].astype(int)
        pred_dt_i = int(nn_iz[best_arg])
        pred_dt_score = scores[best_arg]
        vdist = vdist[best_arg]

    else:
        pred_dt_i = 0
        pred_dt_score = 0.0
        vdist = 0.0

    return (
        np.array([true_dt_i]),
        np.array([pred_dt_i]),
        np.array([pred_dt_score]),
        np.array([vdist]),
    )


def _fit_gain_one_channel(one_channel_radmat, expected_dark_cycle):
    """
    Fit the gain of one_channel_radmat

    Assumes (demands) that the dye count is not more than one.
    This will fail in all other cases.

    Arguments:
        one_channel_radmat: A 2D matrix of (n_samples, n_cycles)
        expected_dark_cycle: cycle (0-based) where the dark is expected

    Returns:
        Gain estimate
    """
    from sklearn.cluster import KMeans  # Defer slow import

    check.array_t(one_channel_radmat, ndim=2)
    n_rows, n_cycles = one_channel_radmat.shape
    assert np.any(one_channel_radmat > 100.0)  # Check that this is a non-unity X

    n_rows = one_channel_radmat.shape[0]

    # Step one, divide the measurements into two groups (dark and bright) by using k-means

    # sklearn's KMeans only accept 2+ dimensional data, so convert it
    samples = one_channel_radmat.flatten()
    samples_2d = np.zeros((samples.shape[0], 2))
    samples_2d[:, 0] = samples
    kmeans = KMeans(n_clusters=2, random_state=0).fit(samples_2d)
    gain = np.median(samples[kmeans.labels_ == 1])
    dark = np.sort(samples[kmeans.labels_ == 1])[0]
    # gain is now an initial guess at the one-dye gain and
    # dark is a lower bound

    # Step 2: Filter outliers and refine estimate

    # Filter out any rows that don't conform to the expected pattern
    # by measuring the distance of each row in pattern space to the expected
    expected_pattern = np.ones((n_cycles,))
    expected_pattern[expected_dark_cycle:] = 0

    keep_mat = None
    for i in range(5):  # 5 is empirical
        # Repeat solving for the gain keeping anything that is < 2 stdev from the gain
        # This knocks out high corruptions
        delta = one_channel_radmat / gain - expected_pattern
        dist = np.sqrt(np.sum(delta * delta, axis=1))

        keep_mask = dist < 1.0
        keep_mat = one_channel_radmat[keep_mask]

        # Take samples from rows that match the pattern and that aren't dark
        samples = keep_mat.flatten()
        samples = samples[samples > dark]

        # Keep samples that are 2 std from current gain
        std = np.std(samples - gain)
        samples = samples[(samples - gain) ** 2 < (2 * std) ** 2]

        gain = np.mean(samples)

    return gain, keep_mat


def _fit_vpd_one_channel(one_channel_radmat, gain, expected_dyerow, accept_dist=1.0):
    n_rows, n_cycles = one_channel_radmat.shape
    assert np.any(one_channel_radmat > 100.0)  # Check that this is a non-unity X
    assert expected_dyerow.shape[0] == n_cycles
    n_dyes = int(np.max(expected_dyerow))

    radmat = one_channel_radmat / gain

    # Filter out any rows that don't conform to the expected pattern
    # by measuring the distance of each row in pattern space to the expected
    delta = radmat - expected_dyerow
    #     debug(delta[:, 0:])
    #     debug(expected_dyerow)
    delta = delta * np.exp(-0.2 * expected_dyerow)
    #    delta[:, expected_dyerow == 0.0] = 0.0
    #     debug(delta[:, 0:])
    dist = np.sqrt(np.sum(delta * delta, axis=1))

    # Take samples from rows that match the pattern
    keep_rows = dist < accept_dist
    keep_radmat = radmat[keep_rows]

    x = [0]
    y = [0]
    for i in range(1, n_dyes + 1):
        cycles_with_this_many_dyes = np.argwhere(expected_dyerow == i).flatten()
        samples = keep_radmat[:, cycles_with_this_many_dyes]
        x += [i]
        y += [np.var(samples)]

    x = np.array(x)
    y = np.array(y)
    m, b = np.polyfit(x, y, 1)

    return m, b, n_dyes, x, y, keep_radmat


def _step_1_create_unit_radmat(channel_i_to_gain, radmat):

    check.array_t(radmat, ndim=3)
    check.array_t(channel_i_to_gain, ndim=1)
    return radmat / channel_i_to_gain[None, :, None]


def _step_2_create_neighbors_lookup(dyemat, true_pep_iz):
    """
    The dyemat may have many duplicate rows, each from some number of peps.

    These duplicate rows are consolidated so that each coordinate in dyemat space
    is given a unique "dye_i".

    Returns:
        dt_mat: The unique (sorted) rows of dyemat
        dyetracks_df: DF(dye_i, weight).
            Where weight is the sum of all rows that pointed to this dyetrack
        dt_pep_sources_df: DF(dye_i, pep_i, n_rows)
            Records how many times each peptide generated dye_i where count > 0.
        flann: A fast Approximate Nearest Neighbors lookup using PYFLANN.
    """
    check.array_t(dyemat, ndim=3)

    dt_mat, true_dt_iz, dt_counts = np.unique(
        dyemat, return_inverse=True, return_counts=True, axis=0
    )

    prune_rare = False
    if prune_rare:
        keep_mask = dt_counts > 0
        keep_mask[0] = True

        dt_mat = dt_mat[keep_mask]
        dt_counts = dt_counts[keep_mask]

        n_old = len(keep_mask)
        n_new = keep_mask.sum()
        orig = np.arange(n_old)
        new_to_old = orig[keep_mask]
        old_to_new = np.zeros((n_old,))
        new_i = np.arange(n_new)
        old_to_new[new_to_old] = new_i
        true_dt_iz = old_to_new[true_dt_iz]

    _, n_channels, n_cycles = dt_mat.shape

    # PREPEND a zero element that represent nul
    dt_mat = np.vstack((np.zeros((1, n_channels, n_cycles)), dt_mat))
    dt_counts = np.concatenate(([0], dt_counts))
    true_dt_iz += 1

    dyetracks_df = (
        pd.DataFrame(dict(weight=dt_counts))
        .reset_index()
        .rename(columns=dict(index="dye_i"))
    )

    dt_pep_sources_df = (
        pd.DataFrame(dict(dye_i=true_dt_iz, pep_i=true_pep_iz))
        .groupby(["dye_i", "pep_i"])
        .size()
        .to_frame("n_rows")
        .reset_index()
    )

    flann = _create_flann(dt_mat)

    return dt_mat, dyetracks_df, dt_pep_sources_df, flann


def _step_3_create_inverse_variances(dt_mat, channel_i_to_vpd):
    """
    Using the Variance Per Dye find the inverse for each row of dyemat.
    This deals with zero-dyes by assigning the half the variance of the 1-count dye.
    vpd stands for variance per dye. Our models indicate that the standard deviation
    goes up roughly linearly with the number of dyes.
    The later code (_do_nn_and_gmm) needs the inverse variance, so we square the standard deviation to obtain the
    variance and take the inverse.
    Arguments:
        dt_mat is the unique dyetracks

    Returns:
        ndarray(n_rows, n_channels * n_cycles): inverse variance for each row (flatten)


    TODO!!!

    This is the wrong model.
    I need to convert this and the gmm code above to use the same
    model that we use for Random Forest.
    I think that this is the culprit as to why N is doing poorly compared
    to RF in cases where there are a larger dye count.

    In particular this is a linear incrase in the VARIANCE as oppososed
    to the stdev. In a simulation I did in explore/normal_vs_lognormal.ipynb
    you can see that a linear increase in the stdev is a much more accurate model
    """

    check.array_t(dt_mat, ndim=3)
    check.array_t(channel_i_to_vpd, ndim=1)
    # Variances of zero will cause div by zeros so all zeros
    # are set to 0.5 which is chosen arbitrarily because it is > 0 and < 1.
    dt_mat = dt_mat.copy()
    dt_mat[dt_mat == 0.0] = 0.5
    vpd_broadcast = channel_i_to_vpd[None, :, None]
    spd = np.sqrt(vpd_broadcast)
    return 1.0 / np.square(
        spd * dt_mat
    )  # Scaling by the standard deviation per dye by channel


def _step_4_gmm_classify(
    radmat,
    dyemat,
    dt_mat,
    dt_inv_var_mat,
    dt_weights,
    flann,
    n_neighbors,
    dt_score_mode,
    dt_filter_threshold,
    dt_score_metric,
    dt_score_bias,
    penalty_coefs,
    rare_penalty,
    radius,
    progress,
):
    """
    The dyemat is passed so that we can get the true_dt_iz for debugging
    """
    check.array_t(radmat, ndim=3)
    true_dt_iz, pred_dt_iz, scores, vdists = zap.arrays(
        _do_nn_and_gmm,
        dict(unit_radrow=radmat, dyerow=dyemat),
        dt_mat=dt_mat,
        dt_inv_var_mat=dt_inv_var_mat,
        dt_weights=dt_weights,
        flann=flann,
        n_neighbors=n_neighbors,
        dt_score_mode=dt_score_mode,
        dt_filter_threshold=dt_filter_threshold,
        dt_score_metric=dt_score_metric,
        dt_score_bias=dt_score_bias,
        penalty_coefs=penalty_coefs,
        rare_penalty=rare_penalty,
        radius=radius,
        _progress=progress,
        _stack=True,
    )

    # I use the dt counts as a weighting factor on the PDFs
    # which means that the scores can be > 1.0.
    # To ensure that all rows get an equal treatment in
    # normalization I simply divide them through by the
    # max value to put them into 0-1 range.

    scores = scores.flatten()
    scores /= np.max(scores)

    return true_dt_iz.flatten(), pred_dt_iz.flatten(), scores, vdists


def _step_5_mle_pred_dt_to_pep(pred_dt_iz, dt_scores, dt_pep_sources_df):
    n_dts = np.max(pred_dt_iz) + 1

    df = dt_pep_sources_df
    df = (
        df.set_index("dye_i")
        .join(
            df.groupby("dye_i").agg(
                sum_n_rows=("n_rows", "sum"), max_n_rows=("n_rows", "max")
            )
        )
        .reset_index()
    )
    df = (
        df.sort_values("n_rows", ascending=False)
        .drop_duplicates("dye_i")
        .set_index("dye_i")
    )

    mle_pep = df.reindex(np.arange(n_dts), fill_value=0)[
        ["n_rows", "sum_n_rows", "pep_i"]
    ].values

    # mle_pep has the most likely pep_i in the 0 column and sum_n_rows in the 1 column
    # so the MLE score is (column 0) over (column 1)
    dye_i_to_pep_score = utils.np_safe_divide(mle_pep[:, 0], mle_pep[:, 1], 0.0)
    dye_i_to_pep_i = mle_pep[:, 2]

    pred_pep_iz = dye_i_to_pep_i[pred_dt_iz]
    pep_scores = dye_i_to_pep_score[pred_dt_iz]

    scores = pep_scores * dt_scores

    assert np.all((0 <= scores) & (scores <= 1.0))

    return pred_pep_iz, pep_scores, scores


def nn(sim_result, dyemat, radmat, nn_params, progress=None):
    """
    Main entrypoint for nearest_neighbors.

    Returns:
        pred_pep_iz
        scores

    This is composed of the following steps:
        1. Create a unit radmat
        2. Create a unique dyetrack mat (dt_mat); these are the
           "neighbors" that will be searched.
        3. Create inverse variance for each row of dt_mat; inv_var_dt_mat
        4. Classify each row of the unit radmat with the Gaussian Mixture Model.
    """

    unit_radmat = _step_1_create_unit_radmat(
        np.array(sim_result.params.channel_i_to_gain), radmat
    )
    dt_mat, dyetracks_df, dt_pep_sources_df, flann = _step_2_create_neighbors_lookup(
        sim_result.unflat("train_dyemat"), sim_result.train_true_pep_iz
    )
    n_dts = dt_mat.shape[0]

    # dt_mat is the dyetrack mat of the TARGETS as build by the training set
    # Not to be confused with dyemat which is the dyemat of the test points
    # There is no guarantee that the dyerow of a test point is even *in*
    # the trainging set.

    inv_var_dt_mat = _step_3_create_inverse_variances(
        dt_mat, np.array(sim_result.params.channel_i_to_vpd)
    )

    dt_weights = dyetracks_df.reindex(np.arange(n_dts), fill_value=0).weight.values

    true_dt_iz, pred_dt_iz, dt_scores, vdists = _step_4_gmm_classify(
        unit_radmat,
        dyemat,
        dt_mat,
        inv_var_dt_mat,
        dt_weights,
        flann,
        nn_params.n_neighbors,
        nn_params.dt_score_mode,
        nn_params.dt_filter_threshold,
        nn_params.dt_score_metric,
        nn_params.dt_score_bias,
        nn_params.penalty_coefs,
        nn_params.rare_penalty,
        nn_params.radius,
        progress,
    )

    assert np.all((0 <= dt_scores) & (dt_scores <= 1.0))

    pred_pep_iz, pep_scores, scores = _step_5_mle_pred_dt_to_pep(
        pred_dt_iz, dt_scores, dt_pep_sources_df
    )

    return Munch(
        unit_radmat=unit_radmat,  # This is really for debugging
        dt_mat=dt_mat,
        dyetracks_df=dyetracks_df,
        dt_pep_sources_df=dt_pep_sources_df,
        true_dt_iz=true_dt_iz,
        pred_dt_iz=pred_dt_iz,
        dt_scores=dt_scores,
        pred_pep_iz=pred_pep_iz,
        pep_scores=pep_scores,
        scores=scores,
        vdists=vdists,
        debug_info=None,
    )
